{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import sys\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "sys.path.append(\"./yoloInference\")\n",
    "from yoloInference.detect import detect_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "seg_session = ort.InferenceSession(\"./bestModels/fcn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color map for segmentation\n",
    "color_map_list = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [25, 82, 255],\n",
    "        [255, 25, 197],\n",
    "        [140, 255, 25],\n",
    "        [0, 0, 0],\n",
    "        [226, 255, 25],\n",
    "        [255, 197, 25],\n",
    "        [140, 25, 255],\n",
    "        [54, 255, 25],\n",
    "        [25, 255, 82],\n",
    "        [255, 111, 25],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def map_one_hot_to_image(one_hot, color_map=color_map_list):\n",
    "    batch_size, height, width, num_colors = one_hot.shape\n",
    "    indices = torch.argmax(one_hot, dim=-1)\n",
    "    output = color_map[indices]\n",
    "    return output\n",
    "\n",
    "\n",
    "def perform_inference(image, color_map=color_map_list):\n",
    "    image = torch.tensor(np.expand_dims(np.array(image), axis=0), dtype=torch.float32)[\n",
    "        :, :, :, :3\n",
    "    ]\n",
    "    resize = transforms.Resize((640, 640))\n",
    "    image = image.permute(0, 3, 2, 1)\n",
    "    image = resize(image)\n",
    "    image = image.permute(0, 3, 2, 1) / 255\n",
    "\n",
    "    input_name = seg_session.get_inputs()[0].name\n",
    "    outputs = seg_session.run(None, {input_name: np.array(image)})\n",
    "    outputs = torch.tensor(outputs[0][0]).permute(1, 2, 0)\n",
    "    outputs = torch.tensor(np.expand_dims(np.array(outputs), axis=0))\n",
    "    outputs = map_one_hot_to_image(outputs, color_map)\n",
    "    return outputs, image\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes, classes, scores):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, cls, score in zip(boxes, classes, scores):\n",
    "        x1, y1, x2, y2 = box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1), f\"Class: {cls}, Score: {score:.2f}\", fill=\"red\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def displayImagesWithBoxesYolo(image_path, bounding_boxes, outputPath=None, show=False):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image using Pillow.\n",
    "\n",
    "    Args:\n",
    "      image_path: Path to the image file.\n",
    "      bounding_boxes: List of bounding boxes in the format (x_center, y_center, width, height).\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Convert bounding boxes to xmin, ymin, xmax, ymax and draw rectangles\n",
    "    for x_center, y_center, width, height in bounding_boxes:\n",
    "        xmin = int((x_center - width / 2) * image.width)\n",
    "        ymin = int((y_center - height / 2) * image.height)\n",
    "        xmax = int((x_center + width / 2) * image.width)\n",
    "        ymax = int((y_center + height / 2) * image.height)\n",
    "\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=\"red\", width=2)\n",
    "    if show:\n",
    "        image.show()\n",
    "    if outputPath:\n",
    "        image.save(outputPath)  # Replace with your desired filename and format\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    # Resize frame to 640x640 and ensure it's RGB\n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Remove alpha channel if present\n",
    "    if frame.shape[2] == 4:\n",
    "        frame = frame[:, :, :3]\n",
    "\n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "\n",
    "    # Perform YOLO detection\n",
    "    temp_path = \"./temp_frame.jpg\"\n",
    "    pil_image.save(temp_path)  # Save using Pillow in RGB format\n",
    "    yolo_output = detect_objects(os.path.abspath(temp_path))[0]\n",
    "    image_with_boxes = pil_image.copy()\n",
    "    if yolo_output:\n",
    "        boxes = [x[1:5] for x in yolo_output]\n",
    "        image_with_boxes = displayImagesWithBoxesYolo(temp_path, boxes)\n",
    "    os.remove(temp_path)\n",
    "\n",
    "    # Convert image_with_boxes to numpy array (already in RGB)\n",
    "    image_with_boxes_np = np.array(image_with_boxes)\n",
    "\n",
    "    # Perform segmentation\n",
    "    segmented_image, _ = perform_inference(pil_image)\n",
    "\n",
    "    # Convert segmented image to numpy array (already in RGB)\n",
    "    segmented_np = cv2.resize(\n",
    "        segmented_image.squeeze().numpy().astype(np.uint8), (640, 640)\n",
    "    )\n",
    "\n",
    "    # Blend original frame with segmentation\n",
    "    alpha = 0.3\n",
    "    blended = cv2.addWeighted(frame, 1 - alpha, segmented_np, alpha, 0)\n",
    "\n",
    "    return blended, image_with_boxes_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame\n",
    "    blended, side_by_side = process_frame(np.array(frame))\n",
    "\n",
    "    # Display the results\n",
    "    cv2.imshow(\"Blended\", cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imshow(\"boxes\", cv2.cvtColor(side_by_side, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inmindProjectEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
